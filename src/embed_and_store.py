import os
import json
import chromadb
from sentence_transformers import SentenceTransformer
from modelscope import snapshot_download   # ä½¿ç”¨modelscope å›½å†…é•œåƒåŠ é€Ÿæ¨¡åž‹ä¸‹è½½
from chromadb import errors

def getFullPath(*paths):
    """èŽ·å–é¡¹ç›®å†…çš„å®Œæ•´è·¯å¾„"""
    projetc_path = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(projetc_path)
    return os.path.join(project_root,*paths)

def main():
    # 1.åŠ è½½æ–‡æœ¬å—æ•°æ®
    chunk_data = getFullPath("data","chunks.json")
    with open(chunk_data,"r",encoding="utf-8") as f:
        chunks = json.load(f)
    texts = [c['text'] for c in chunks]
    ids = [str(c['id']) for c in chunks]
    print(f'ðŸ“„ åŠ è½½{len(texts)}ä¸ªæ–‡æœ¬å—')

    # 2. åŠ è½½BGEä¸­æ–‡Embeddingæ¨¡åž‹
    print("â³ æ­£åœ¨åŠ è½½BGEæ¨¡åž‹...")
    # # åŠ è½½é¢„è®­ç»ƒæ¨¡åž‹
    # model=SentenceTransformer(
    #     'BAAI/bge-large-zh-v1.5',
    #     trust_remote_code=True  ##  å…è®¸åŠ è½½è‡ªå®šä¹‰æ¨¡åž‹ä»£ç 
    # ) 
    # model.max_seq_length = 512 # æå‡é•¿æ–‡æœ¬å¤„ç†èƒ½åŠ›

    # å›½å†…é•œåƒä¸‹è½½æ¨¡åž‹ 
    # ä¸‹è½½åˆ°æœ¬åœ°
    model_dir = snapshot_download(
        'BAAI/bge-large-zh-v1.5',
        cache_dir="D:/aiCache"    
    )
    print(f"æ¨¡åž‹ä¸‹è½½è·¯å¾„{model_dir}")
    model = SentenceTransformer(model_dir)
    model.max_seq_length = 512

    # 3. ç”Ÿæˆembedding
    print("æ­£åœ¨ç”Ÿæˆembding...")
    embeddings = model.encode(texts,normalize_embeddings=True, show_progress_bar= True)
    print(f"ç”Ÿæˆ{len(embeddings)}ä¸ªå‘é‡ï¼Œç»´åº¦ï¼š{len(embeddings[0])}")

    # 4. åˆå§‹åŒ–chromaå¹¶å­˜å…¥
    vector_db_path = getFullPath("data","vector_db")
    client = chromadb.PersistentClient(path=vector_db_path)

    # åˆ é™¤å·²å­˜åœ¨çš„é›†åˆï¼Œé¿å…é‡å¤
    collection_name = "finreg_chunks"
    try:
        client.delete_collection("finreg_chunks")
    except errors.NotFoundError:
        pass # å¦‚æžœä¸å­˜åœ¨åˆ™ç›´æŽ¥è·³è¿‡

    # åˆ›å»ºæ–°é›†åˆ  chroma ä¼šè‡ªåŠ¨å­˜å‚¨åŽŸå§‹æ•°æ® + å‘é‡
    collection = client.create_collection(
        name=collection_name,
        metadata={"hnsw:space": "cosine"} # ä½™å¼¦ç›¸ä¼¼åº¦
    )

    # æ‰¹é‡æ·»åŠ 
    collection.add(
        ids = ids,
        documents =  texts,
        embeddings = embeddings.tolist() # å°†listè½¬ä¸ºjson åµŒå…¥å‘é‡å‡½æ•° ç”Ÿæˆæ–‡æœ¬çš„å‘é‡
    )
    print(f"å‘é‡æ•°æ®å·²ä¿å­˜è‡³ï¼š{vector_db_path}")

    # ç®€å•æ£€ç´¢æµ‹è¯•
    print("\n æµ‹è¯•æ£€ç´¢ eastå®¢æˆ·idå­—æ®µçš„è¦æ±‚")
    query = 'EASTå®¢æˆ·idå­—æ®µè¦æ±‚'
    query_embedding = model.encode(
        query
        ,normalize_embeddings=True # ç¡®ä¿ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—æ˜¯å‡†ç¡®çš„
     ).tolist()

    results = collection.query(
        query_embeddings=[query_embedding],
        n_results=3
    )

    for i,doc in enumerate(results["documents"][0]):
        print(f"\n ç»“æžœ{i+1}:")
        print(doc[:200] + "..." if len(doc)>200 else doc)

if __name__ == "__main__":
    main()